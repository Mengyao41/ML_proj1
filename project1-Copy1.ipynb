{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOyOkJJA7Foy"
   },
   "source": [
    "# Machine Learning in Python - Group Project 1\n",
    "\n",
    "**Due Friday, March 10th by 16.00 pm.**\n",
    "\n",
    "*include contributors names here (such as Name1, Name2, ...)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqPpPRE37Fo0"
   },
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: schrutepy in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (0.1.3)\n",
      "Requirement already satisfied: ipython>=6 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from schrutepy) (7.29.0)\n",
      "Requirement already satisfied: wheel in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from schrutepy) (0.37.0)\n",
      "Requirement already satisfied: pip in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from schrutepy) (21.2.4)\n",
      "Requirement already satisfied: twine>=3 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from schrutepy) (4.0.2)\n",
      "Requirement already satisfied: pandas in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from schrutepy) (1.3.4)\n",
      "Requirement already satisfied: decorator in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from ipython>=6->schrutepy) (5.1.0)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from ipython>=6->schrutepy) (0.1.2)\n",
      "Requirement already satisfied: pygments in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from ipython>=6->schrutepy) (2.14.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from ipython>=6->schrutepy) (58.0.4)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from ipython>=6->schrutepy) (5.1.0)\n",
      "Requirement already satisfied: pickleshare in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from ipython>=6->schrutepy) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from ipython>=6->schrutepy) (3.0.20)\n",
      "Requirement already satisfied: appnope in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from ipython>=6->schrutepy) (0.1.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from ipython>=6->schrutepy) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from ipython>=6->schrutepy) (0.18.0)\n",
      "Requirement already satisfied: backcall in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from ipython>=6->schrutepy) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6->schrutepy) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6->schrutepy) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6->schrutepy) (0.2.5)\n",
      "Requirement already satisfied: rfc3986>=1.4.0 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from twine>=3->schrutepy) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from twine>=3->schrutepy) (4.8.1)\n",
      "Requirement already satisfied: rich>=12.0.0 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from twine>=3->schrutepy) (13.3.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from twine>=3->schrutepy) (1.26.7)\n",
      "Requirement already satisfied: keyring>=15.1 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from twine>=3->schrutepy) (23.1.0)\n",
      "Requirement already satisfied: requests-toolbelt!=0.9.0,>=0.8.0 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from twine>=3->schrutepy) (0.10.1)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from twine>=3->schrutepy) (2.26.0)\n",
      "Requirement already satisfied: pkginfo>=1.8.1 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from twine>=3->schrutepy) (1.9.6)\n",
      "Requirement already satisfied: readme-renderer>=35.0 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from twine>=3->schrutepy) (37.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=3.6->twine>=3->schrutepy) (3.6.0)\n",
      "Requirement already satisfied: docutils>=0.13.1 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from readme-renderer>=35.0->twine>=3->schrutepy) (0.17.1)\n",
      "Requirement already satisfied: bleach>=2.1.0 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from readme-renderer>=35.0->twine>=3->schrutepy) (4.0.0)\n",
      "Requirement already satisfied: webencodings in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from bleach>=2.1.0->readme-renderer>=35.0->twine>=3->schrutepy) (0.5.1)\n",
      "Requirement already satisfied: packaging in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from bleach>=2.1.0->readme-renderer>=35.0->twine>=3->schrutepy) (21.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from bleach>=2.1.0->readme-renderer>=35.0->twine>=3->schrutepy) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->twine>=3->schrutepy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->twine>=3->schrutepy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->twine>=3->schrutepy) (2021.10.8)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from rich>=12.0.0->twine>=3->schrutepy) (2.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=12.0.0->twine>=3->schrutepy) (0.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from packaging->bleach>=2.1.0->readme-renderer>=35.0->twine>=3->schrutepy) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from pandas->schrutepy) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from pandas->schrutepy) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from pandas->schrutepy) (1.20.3)\n",
      "Requirement already satisfied: nltk in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (3.6.5)\n",
      "Requirement already satisfied: click in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\n",
      "\u001b[K     |████████████████████████████████| 226 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn>=1.0.2\n",
      "  Downloading scikit_learn-1.2.1-cp39-cp39-macosx_10_9_x86_64.whl (9.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1 MB 1.4 MB/s eta 0:00:01     |█▊                              | 501 kB 896 kB/s eta 0:00:10\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (1.7.1)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /Users/abohane/opt/anaconda3/lib/python3.9/site-packages (from imbalanced-learn) (2.2.0)\n",
      "Installing collected packages: joblib, scikit-learn, imbalanced-learn\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "Successfully installed imbalanced-learn-0.10.1 joblib-1.2.0 scikit-learn-1.2.1\n"
     ]
    }
   ],
   "source": [
    "# From the instructions, we can see that the data is taken from this library (but then modified) so we install this\n",
    "# package and use the data for feature engineering!\n",
    "!pip install schrutepy\n",
    "!pip install nltk\n",
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jb4qh-pi7Fo1"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_OneToOneFeatureMixin' from 'sklearn.base' (/Users/abohane/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# scikit-learn >= 1.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneToOneFeatureMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OneToOneFeatureMixin' from 'sklearn.base' (/Users/abohane/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/61/n2sfq6s16zdgx5_xwt6gtdvr0000gn/T/ipykernel_16725/2720146320.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPowerTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPowerTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# process, as it may not be compiled yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     from . import (\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mcombine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mensemble\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/imblearn/combine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_enn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_tomek\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/imblearn/combine/_smote_enn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseOverSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneToOneFeatureMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_OneToOneFeatureMixin\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mOneToOneFeatureMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_OneToOneFeatureMixin' from 'sklearn.base' (/Users/abohane/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py)"
     ]
    }
   ],
   "source": [
    "# Add any additional libraries or submodules below\n",
    "\n",
    "# Data libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from schrutepy import schrutepy\n",
    "import itertools\n",
    "\n",
    "# Sentiment Analysis\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# download the vader lexicon \n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "\"\"\"Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for\n",
    "Sentiment Analysis of Social Media Text. Eighth International Conference on\n",
    "Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\"\"\"\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "\n",
    "# sklearn modules that are necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hjs5785u7Fo1"
   },
   "outputs": [],
   "source": [
    "# First we load the data \n",
    "data = pd.read_csv(\"the_office.csv\")\n",
    "# and our external data from SchrutePy. (Referenced at end)\n",
    "data_from_schrutepy = schrutepy.load_schrute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making sure that all the necessary libraries or submodules are uploaded here, please follow the given skeleton to create your project report. \n",
    "- Your completed assignment must follow this structure \n",
    "- You should not add or remove any of these sections, if you feel it is necessary you may add extra subsections within each (such as *2.1. Encoding*). \n",
    "\n",
    "**Do not forget to remove the instructions for each section in the final document.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K314dGEL7Fo1"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "*This section should include a brief introduction to the task and the data (assume this is a report you are delivering to a client).* \n",
    "\n",
    "- If you use any additional data sources, you should introduce them here and discuss why they were included.\n",
    "\n",
    "- Briefly outline the approaches being used and the conclusions that you are able to draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FINISH'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"FINISH\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3P6Vdzbo7Fo2"
   },
   "source": [
    "## 2. Exploratory Data Analysis and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Arg9_dYE7Fo2"
   },
   "source": [
    "*Include a detailed discussion of the data with a particular emphasis on the features of the data that are relevant for the subsequent modeling.* \n",
    "\n",
    "- Including visualizations of the data is strongly encouraged - all code and plots must also be described in the write up. \n",
    "- Think carefully about whether each plot needs to be included in your final draft - your report should include figures but they should be as focused and impactful as possible.\n",
    "\n",
    "*Additionally, this section should also implement and describe any preprocessing / feature engineering of the data.*\n",
    "\n",
    "- Specifically, this should be any code that you use to generate new columns in the data frame `d`. All of this processing is explicitly meant to occur before we split the data in to training and testing subsets. \n",
    "- Processing that will be performed as part of an sklearn pipeline can be mentioned here but should be implemented in the following section.*\n",
    "\n",
    "**All code and figures should be accompanied by text that provides an overview / context to what is being done or presented.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will explore the given dataset and show what the SchrutePy dataset looks like, which we downloaded from the package SchrutePy (reference below). This was hinted at in the instructions and will give us extra data to make our model.\n",
    "\n",
    "Below is the given dataset from the_office.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>director</th>\n",
       "      <th>writer</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>air_date</th>\n",
       "      <th>n_lines</th>\n",
       "      <th>n_directions</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_speak_char</th>\n",
       "      <th>main_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>Ken Kwapis</td>\n",
       "      <td>Ricky Gervais;Stephen Merchant;Greg Daniels</td>\n",
       "      <td>7.6</td>\n",
       "      <td>3706</td>\n",
       "      <td>2005-03-24</td>\n",
       "      <td>229</td>\n",
       "      <td>27</td>\n",
       "      <td>2757</td>\n",
       "      <td>15</td>\n",
       "      <td>Angela;Dwight;Jim;Kevin;Michael;Oscar;Pam;Phyl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Diversity Day</td>\n",
       "      <td>Ken Kwapis</td>\n",
       "      <td>B.J. Novak</td>\n",
       "      <td>8.3</td>\n",
       "      <td>3566</td>\n",
       "      <td>2005-03-29</td>\n",
       "      <td>203</td>\n",
       "      <td>20</td>\n",
       "      <td>2808</td>\n",
       "      <td>12</td>\n",
       "      <td>Angela;Dwight;Jim;Kelly;Kevin;Michael;Oscar;Pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  episode   episode_name    director  \\\n",
       "0       1        1          Pilot  Ken Kwapis   \n",
       "1       1        2  Diversity Day  Ken Kwapis   \n",
       "\n",
       "                                        writer  imdb_rating  total_votes  \\\n",
       "0  Ricky Gervais;Stephen Merchant;Greg Daniels          7.6         3706   \n",
       "1                                   B.J. Novak          8.3         3566   \n",
       "\n",
       "     air_date  n_lines  n_directions  n_words  n_speak_char  \\\n",
       "0  2005-03-24      229            27     2757            15   \n",
       "1  2005-03-29      203            20     2808            12   \n",
       "\n",
       "                                          main_chars  \n",
       "0  Angela;Dwight;Jim;Kevin;Michael;Oscar;Pam;Phyl...  \n",
       "1  Angela;Dwight;Jim;Kelly;Kevin;Michael;Oscar;Pa...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the external dataset we use from SchrutePy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>director</th>\n",
       "      <th>writer</th>\n",
       "      <th>character</th>\n",
       "      <th>text</th>\n",
       "      <th>text_w_direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>Ken Kwapis</td>\n",
       "      <td>Ricky Gervais;Stephen Merchant;Greg Daniels</td>\n",
       "      <td>Michael</td>\n",
       "      <td>All right Jim. Your quarterlies look very good...</td>\n",
       "      <td>All right Jim. Your quarterlies look very good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilot</td>\n",
       "      <td>Ken Kwapis</td>\n",
       "      <td>Ricky Gervais;Stephen Merchant;Greg Daniels</td>\n",
       "      <td>Jim</td>\n",
       "      <td>Oh, I told you. I couldn't close it. So...</td>\n",
       "      <td>Oh, I told you. I couldn't close it. So...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  season  episode episode_name    director  \\\n",
       "0      1       1        1        Pilot  Ken Kwapis   \n",
       "1      2       1        1        Pilot  Ken Kwapis   \n",
       "\n",
       "                                        writer character  \\\n",
       "0  Ricky Gervais;Stephen Merchant;Greg Daniels   Michael   \n",
       "1  Ricky Gervais;Stephen Merchant;Greg Daniels       Jim   \n",
       "\n",
       "                                                text  \\\n",
       "0  All right Jim. Your quarterlies look very good...   \n",
       "1         Oh, I told you. I couldn't close it. So...   \n",
       "\n",
       "                                    text_w_direction  \n",
       "0  All right Jim. Your quarterlies look very good...  \n",
       "1         Oh, I told you. I couldn't close it. So...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_from_schrutepy.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a quick overview of what our columns are, how the data is inputted and how the overall structure of the data is. This is important for us so we can create efficient code later!\n",
    "Now lets quickly describe our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <td>186.0</td>\n",
       "      <td>5.46</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode</th>\n",
       "      <td>186.0</td>\n",
       "      <td>12.48</td>\n",
       "      <td>7.23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imdb_rating</th>\n",
       "      <td>186.0</td>\n",
       "      <td>8.25</td>\n",
       "      <td>0.54</td>\n",
       "      <td>6.7</td>\n",
       "      <td>7.90</td>\n",
       "      <td>8.2</td>\n",
       "      <td>8.60</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_votes</th>\n",
       "      <td>186.0</td>\n",
       "      <td>2129.54</td>\n",
       "      <td>790.79</td>\n",
       "      <td>1393.0</td>\n",
       "      <td>1628.50</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>2385.00</td>\n",
       "      <td>7934.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_lines</th>\n",
       "      <td>186.0</td>\n",
       "      <td>296.40</td>\n",
       "      <td>82.00</td>\n",
       "      <td>131.0</td>\n",
       "      <td>255.25</td>\n",
       "      <td>281.0</td>\n",
       "      <td>314.50</td>\n",
       "      <td>625.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_directions</th>\n",
       "      <td>186.0</td>\n",
       "      <td>50.15</td>\n",
       "      <td>23.94</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.00</td>\n",
       "      <td>46.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_words</th>\n",
       "      <td>186.0</td>\n",
       "      <td>3053.51</td>\n",
       "      <td>799.27</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>2670.25</td>\n",
       "      <td>2872.5</td>\n",
       "      <td>3141.00</td>\n",
       "      <td>6076.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_speak_char</th>\n",
       "      <td>186.0</td>\n",
       "      <td>20.69</td>\n",
       "      <td>5.09</td>\n",
       "      <td>12.0</td>\n",
       "      <td>17.00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>23.00</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count     mean     std     min      25%     50%      75%     max\n",
       "season        186.0     5.46    2.40     1.0     3.00     6.0     7.75     9.0\n",
       "episode       186.0    12.48    7.23     1.0     6.00    12.0    18.00    28.0\n",
       "imdb_rating   186.0     8.25    0.54     6.7     7.90     8.2     8.60     9.7\n",
       "total_votes   186.0  2129.54  790.79  1393.0  1628.50  1954.0  2385.00  7934.0\n",
       "n_lines       186.0   296.40   82.00   131.0   255.25   281.0   314.50   625.0\n",
       "n_directions  186.0    50.15   23.94    11.0    34.00    46.0    60.00   166.0\n",
       "n_words       186.0  3053.51  799.27  1098.0  2670.25  2872.5  3141.00  6076.0\n",
       "n_speak_char  186.0    20.69    5.09    12.0    17.00    20.0    23.00    54.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data.describe().T.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have 186 episodes in total, average rating of 8.25 (good show), several other parameters which will be important later when we feature engineer. We can see for example there is quite a large standard deviation in the amount of votes, which could skew our ratings. We also see our rating (target variable) has a standard deviation of 0.54, whereas the mean is 8.25. This shows there is enough spread of rating to make non-uniform predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame: data, has  0  NaN values\n"
     ]
    }
   ],
   "source": [
    "print('The DataFrame: data, has ',data.isna().sum().sum(),' NaN values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about our external dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_schrutepy.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have the columns text and text_w_directions have NaN values. However, we will not be using these anyways since its just the script! So we do not have to worry about it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will no get our numerical data so we can visualize it:\n",
    "num_data = data.copy()\n",
    "num_data = num_data.drop(columns = ['episode_name', 'director', 'writer', 'air_date', 'main_chars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at the distribution of the columns. (except of course the season number, episode, and air date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(15,7), ncols=3, nrows=2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "#define our columns\n",
    "lst = ['imdb_rating','total_votes', 'n_lines', 'n_directions','n_words', 'n_speak_char']\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.histplot(data = num_data[lst[i]], bins=30, ax=ax)\n",
    "    ax.set_title(num_data[lst[i]].name)\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the distribution is important because it lets us see several hints such as the Skewness, Gaussian-ness, etc. We can see the IMDB rating follows what seems like a normal distribution, whereas total votes decreasing seemingly exponentially. While the mathematical relation isn't necessarily important for each of the variables, it is important to understand total votes is skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs above allow us to visualise the distributions of all of our numerical features. We can see that some features are skewed and others follow a normal gaussian distribution. From this information, we will implement a box cox transformation to transform all non-gaussian features to gaussian. This will be done in the feature engineering section. Doing so will improve the predictions made by our linear regression model further down the line. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before we move on to visualizing the correlation between variables, we want to see a breakdown of the relationship between writers and directors. After group discussion, we already know we want to include writers and directors in our model, but we need to make encoding of these variables. \n",
    "\n",
    "What is important to us is not the directors which had one hit wonders: the directors which had directed one episode and got lucky with a good rating. Maybe they are really good, but we are looking for consistency. The director for our final episode has to be consistently good.\n",
    "\n",
    "<b> We set a threshold to choose which directors to encode in our model:</b> we want directors to have directed at least ***5 shows*** and then from there select the ***best 5***  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get director dataframe which has their ratings and get a new column calculating the amount of times they directed\n",
    "director = data[['director', 'imdb_rating']].reindex(columns=list(['director', 'imdb_rating', 'freq']), fill_value=1)\n",
    "#calculates the sum\n",
    "director = director.groupby('director')[['director', 'imdb_rating', 'freq']].sum() \n",
    "director['mean_rating'] = round(director['imdb_rating'] / director['freq'], 2)\n",
    "# and sort the values for us on frequency (explained below) with a mean rating in the 75% percentile. \n",
    "director[director['freq']>=5].drop(columns = 'imdb_rating').sort_values(by='mean_rating', ascending=False).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that using this criteria, we get some of the consistnently best directors. Fortunately for us, we can see that all our directors with high frequency also have fairly good ratings, with Paul Feig having a very high rating consistently! We see that Jeffery Blitz has the same rating as Charles McDougall so we use Jeffery since he has a higher frequency!\n",
    "\n",
    "Now lets do the same with writer. Here some times the writers collab. We will NOT split up the writers individually because we believe collaborations between writers can lead to better results than the writers individually together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get director dataframe which has their ratings and get a new column calculating the amount of times they directed\n",
    "writer = data[['writer', 'imdb_rating']].reindex(columns=list(['writer', 'imdb_rating', 'freq']), fill_value=1)\n",
    "#calculates the sum\n",
    "writer = writer.groupby('writer')[['writer', 'imdb_rating', 'freq']].sum() \n",
    "writer['mean_rating'] = round(writer['imdb_rating'] / writer['freq'], 2)\n",
    "# and sort the values for us on frequency (explained below) with a mean rating in the 75% percentile. \n",
    "writer[writer['freq']>=5].drop(columns = 'imdb_rating').sort_values(by='mean_rating', ascending=False).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same reasoning, we can find the best consistently best 5 writers which we will choose to encode!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done all of this: It is time to see the correlation between our variables so we can choose the best selection for our model to reduce feature dependence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap\n",
    "sns.set(rc={'figure.figsize': (14, 8)})\n",
    "sns.heatmap(num_data.corr(), annot = True, fmt = '.2f', linewidths = 2)\n",
    "plt.title(\"Correlation Heatmap for all \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some very important things to reduce colinearity/codependence:\n",
    "\n",
    "* n_lines and n_words is very highly correlated (obviously) so we will deal with that later. Other n_* lines are also slightly correlated so we will find a way to deal with those too.\n",
    "* imbd_rating and total_votes is also very highly correlated. \n",
    "    + We need to be careful here because this implies episodes with a large number of votes often is rated more highly. We cannot have number of votes as an input variable in our model because for a reunion episode, we obviosuly cannot say a priori the amount of votes we will get. To deal with this we will look deeper into the problem: We can adjust the imdb rating based on the amount of votes it got or choose to ignore the correlation if we find the relationship to be insignificant. More on this later...\n",
    "* The same is with rating and season number. However, since we cannot account for the season number in our reunion episode, we cannot do anything about this either. We could weight each season to imbd rating but do not believe this to be outside the scope of this project. It would be great for further research!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, lets look at how we can deal with number of ____ feautures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now want to see how the highly correlated values are relates:\n",
    "sns.pairplot(data[['n_lines','n_directions','n_words', 'n_speak_char']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a very clear __linear__ relationship between n_lines and n_words which is good for us since it means we can combine or eliminate one of those feautres. The other variables we do not see as strong relationship so we are hesitant to eliminate those. We will see if testing the model on a reduced parameter space will improve performance later, in which we can reduce some of these feautures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***FIX/Adjust BELOW***</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features we are eliminating (from the_office.csv) and why:\n",
    "* We will eliminate the column n_words since we found that n_lines and n_words were highly linearly correlated. Doing so will reduce collinearity in our final model. \n",
    "* We will remove season number, episode number and air_date because in our opinion, this should not have an affect on the reunion episode. We consider our personal experience and realize most of the office nowadays is watched through streaming services, which are independent of air date. We do however acknowledge that the rating could be dependent on air_time and season numbers, although marginally\n",
    "\n",
    "#### Features we will engineer:\n",
    "* We want to see how many times do the main characters talk during the episode and see if it effects the rating.\n",
    "* We want to OneHotEncode the writers and directors\n",
    "* OneHotEncode\n",
    "\n",
    "#### Will we adjust the IMDB rating according to the number of votes and season number to evenly weight the ratings for our model?\n",
    "As a group, we have decided to _______\n",
    "\n",
    "#### Finally, we will standardise all of our final numerical features\n",
    "This includes standardising the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 How many times do characters speak?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get this data from our external source: data_from_schrutepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data which is how many times does {Character} speak in each episode?\n",
    "speak_count = pd.DataFrame(data_from_schrutepy.groupby(by=['season','episode','character']).count()['text']).unstack()\n",
    "speak_count.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we now have 773 characters in the office, most of which do not speak: a lot of minor characters we do not wish to include in our model. So now lets filter the main characters only: this can be found in the the_office.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlate this to the main characters in this episode (from the_office.csv) and delete all minor characters\n",
    "a = data.main_chars.str.split(';')\n",
    "#this gets all the unique main characters from the_office.csv and save it as main_characters\n",
    "main_characters = list(itertools.chain.from_iterable(a))\n",
    "main_characters = pd.DataFrame(main_characters).drop_duplicates().reset_index()[0]\n",
    "\n",
    "# adjust the dataFrame index\n",
    "speak_count = speak_count.reset_index(names = ['episode','season', 'character'], col_level=1)\n",
    "# get just the main character\n",
    "speak_count_main_characters = speak_count['text'][main_characters].fillna(0)\n",
    "speak_count_main_characters.head()\n",
    "#then add the main characters to the model input data!\n",
    "data[speak_count_main_characters.columns] = speak_count_main_characters\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this worked out really well! We now have extra columns which shows how often the main characters speak in the show per episode!\n",
    "\n",
    "Below we can show a visual representation of how many times each main character speaks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data[main_characters])\n",
    "plt.xticks(rotation=45) # rotate x-labels 45 degrees\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are a lot of outliers. This shows some episodes focus on specific characters\n",
    "\n",
    "    ***REMOVE EPISODES WITH A FOCUS ON ONE CHARACTER*** AS further expansion on project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 One Hot Encode the writers and directors:\n",
    "\n",
    "It is now time to one hot encode the important writers and directors we found in the EDA section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define the directors and writers we are looking \n",
    "directors = ['Paul Feig','Ken Kwapis','Greg Daniels','Ken Whittingham','Jeffrey Blitz']\n",
    "writers = ['Greg Daniels','Gene Stupnitsky;Lee Eisenberg','Michael Schur','Paul Lieberstein','B.J. Novak']\n",
    "\n",
    "# perform encoding on the director column\n",
    "encoded = pd.get_dummies(data['director'].apply(lambda x: x if x in directors else 'Other_director'))\n",
    "\n",
    "# merge the encoded dataframe with the original dataframe\n",
    "data = pd.concat([data, encoded], axis=1)\n",
    "\n",
    "# drop the original director column\n",
    "data.drop('director', axis=1, inplace=True)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets do the same but with the writers!\n",
    "# perform encoding on the director column\n",
    "encoded = pd.get_dummies(data['writer'].apply(lambda x: x if x in writers else 'Other_writer'))\n",
    "\n",
    "# merge the encoded dataframe with the original dataframe\n",
    "data = pd.concat([data, encoded], axis=1)\n",
    "\n",
    "# drop the original director column\n",
    "data.drop('writer', axis=1, inplace=True)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Dropping our Unessecary Columns\n",
    "\n",
    "Its now time to get rid of season, episode,episode_name,total_votes\tand air_date so we can start to deal with the numbers of words and votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['season', 'episode', \n",
    "                          'episode_name','total_votes',\n",
    "                         'air_date', 'main_chars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 What to do with n_lines and n_words\n",
    "\n",
    "We know from our correlation matrix that these two are highly linearly correlated. That is why we will drop the n_words column (we could also have dropped n_lines instead). \n",
    "\n",
    "We will also know create a copy of our dataFrame to start working on the model with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['n_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will do some combination feature engineering and combine n_lines with the number of lines that a specific character speaks in each episode. To do this we will divide the number of lines each character speaks by the number of lines in each episode. Then we will remove the n_lines column after combination. The resultant metric can be thought of as the character presence and qualitatively represents the percentage of total lines that a specific main character speaks per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:,4:21] = data.iloc[:,4:21].div(data.iloc[:,1], axis=0)\n",
    "data = data.drop(\"n_lines\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5 Sentiment Score\n",
    "\n",
    "We now will run the sentiment analysis on the script per episode. This uses the Vader dictionary to determine how happy/neutral/sad a given script is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# define some text to analyze (fill the silences (nan) with  ' ')\n",
    "text = data_from_schrutepy.fillna(' ').groupby(['season', 'episode'])['text'].apply(' '.join)\n",
    "\n",
    "# analyze the sentiment of the text\n",
    "scores = [analyzer.polarity_scores(i) for i in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the neutral, positive and negative sentiment scores\n",
    "data['pos_score'] = [scores[i]['pos'] for i in range(len(scores))]\n",
    "data['neg_score'] = [scores[i]['neg'] for i in range(len(scores))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature Transformation\n",
    "\n",
    "The final step is to transform our data (using yeo-johnson) to improve our data normality and symmetry! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeo = PowerTransformer()\n",
    "data_scaled = yeo.fit_transform(data)\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df will be the dataframe we are working with now\n",
    "df = data.copy()\n",
    "df_scaled = data_scaled.copy()\n",
    "\n",
    "df.to_csv('final_office.csv')\n",
    "df_scaled.to_csv('final_office_scaled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad7J-Yw67Fo3"
   },
   "source": [
    "## 3. Model Fitting and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXdvOaZs7Fo3"
   },
   "source": [
    "*In this section you should detail your choice of model and describe the process used to refine and fit that model.*\n",
    "\n",
    "- You are strongly encouraged to explore many different modeling methods (e.g. linear regression, regression trees, lasso, etc.) but you should not include a detailed narrative of all of these attempts. \n",
    "- At most this section should mention the methods explored and why they were rejected - most of your effort should go into describing the model you are using and your process for tuning and validatin it.\n",
    "\n",
    "*For example if you considered a linear regression model, a classification tree, and a lasso model and ultimately settled on the linear regression approach then you should mention that other two approaches were tried but do not include any of the code or any in depth discussion of these models beyond why they were rejected. This section should then detail is the development of the linear regression model in terms of features used, interactions considered, and any additional tuning and validation which ultimately led to your final model.* \n",
    "\n",
    "**This section should also include the full implementation of your final model, including all necessary validation. As with figures, any included code must also be addressed in the text of the document.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In GIT there is the final csv file called \n",
    "filneame = './final_office.csv'\n",
    "# in case you guys do not want to run the entire sections above\n",
    "\n",
    "# to inverse the scaling we used: after prediction do: \n",
    "# yeo.inverse_transform(prediction_data)\n",
    "# and be careful! of the column names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5, shuffle = True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_scores(score_dict):\n",
    "    df = pd.DataFrame(score_dict)\n",
    "    df.loc['mean'] = df.mean()\n",
    "    df.loc['sd'] = df.std()\n",
    "    df.rename({\"test_score\":\"val_score\"}, axis=1, inplace=True)\n",
    "    df.index.name = \"fold\"\n",
    "    return df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(m, X, y, plot = False):\n",
    "    \"\"\"Returns the mean squared error, root mean squared error and R^2 value of a fitted model based \n",
    "    on provided X and y values.\n",
    "    \n",
    "    Args:\n",
    "        m: sklearn model object\n",
    "        X: model matrix to use for prediction\n",
    "        y: outcome vector to use to calculating rmse and residuals\n",
    "        plot: boolean value, should fit plots be shown \n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = m.predict(X)\n",
    "    MSE = mean_squared_error(y, y_hat)\n",
    "    RMSE = np.sqrt(mean_squared_error(y, y_hat))\n",
    "    Rsqr = r2_score(y, y_hat)\n",
    "    \n",
    "    Metrics = (round(MSE, 4), round(RMSE, 4), round(Rsqr, 4))\n",
    "    \n",
    "    res = pd.DataFrame(\n",
    "        data = {'y': y, 'y_hat': y_hat, 'resid': y - y_hat}\n",
    "    )\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        sns.lineplot(x='y', y='y_hat', color=\"grey\", data =  pd.DataFrame(data={'y': [min(y),max(y)], 'y_hat': [min(y),max(y)]}))\n",
    "        sns.scatterplot(x='y', y='y_hat', data=res).set_title(\"Actual vs Fitted plot\")\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        sns.scatterplot(x='y_hat', y='resid', data=res).set_title(\"Fitted vs Residual plot\")\n",
    "        plt.hlines(y=0, xmin=np.min(y), xmax=np.max(y), linestyles='dashed', alpha=0.3, colors=\"black\")\n",
    "        \n",
    "        plt.subplots_adjust(left=0.0)\n",
    "        \n",
    "        plt.suptitle(\"Model (MSE, RMSE, Rsqr) = \" + str(Metrics), fontsize=14)\n",
    "        plt.show()\n",
    "    \n",
    "    return MSE, RMSE, Rsqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(m):\n",
    "    \"\"\"Returns the model coefficients from a Scikit-learn model object as an array,\n",
    "    includes the intercept if available.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If pipeline, use the last step as the model\n",
    "    if (isinstance(m, sklearn.pipeline.Pipeline)):\n",
    "        m = m.steps[-1][1]\n",
    "    \n",
    "    \n",
    "    if m.intercept_ is None:\n",
    "        return m.coef_\n",
    "    \n",
    "    return np.concatenate([[m.intercept_], m.coef_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_office_scaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = df.drop(['Unnamed: 0', 'imdb_rating', 'Other_director', 'Other_writer', 'n_directions', 'n_speak_char'], axis = 1)\n",
    "ys = df['imdb_rating']\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, ys, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find Ridge's optimal alpha\n",
    "alphas = np.linspace(0, 400, num=200)\n",
    "gs = GridSearchCV(\n",
    "    make_pipeline(\n",
    "    linear_model.Ridge()),\n",
    "    param_grid = {'ridge__alpha': alphas},\n",
    "    cv = KFold(5, shuffle = True, random_state = 42),\n",
    "    scoring = \"neg_root_mean_squared_error\"\n",
    ")\n",
    "\n",
    "gs.fit(Xs_train, ys_train)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ridge = 215.0753768844221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find lasso's optimal alpha\n",
    "alphas = np.logspace(-3, 1, num=200)\n",
    "gs = GridSearchCV(\n",
    "    make_pipeline(\n",
    "    linear_model.Lasso()),\n",
    "    param_grid = {'lasso__alpha': alphas},\n",
    "    cv = KFold(5, shuffle = True, random_state = 42),\n",
    "    scoring = \"neg_root_mean_squared_error\"\n",
    ")\n",
    "\n",
    "gs.fit(Xs_train, ys_train)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lasso = 0.1289890261253308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with optimal alpha\n",
    "# Description of different models via pipeline\n",
    "\n",
    "linear_pipe = LinearRegression()\n",
    "knn_pipe = KNeighborsRegressor(n_neighbors=3)\n",
    "lasso_pipe = linear_model.Lasso(alpha = best_lasso)\n",
    "ridge_pipe = linear_model.Ridge(alpha = best_ridge)\n",
    "\n",
    "\n",
    "# Collection of all models\n",
    "model_dict = {\n",
    "    'LinearRegression': linear_pipe,\n",
    "    'KNN': knn_pipe,\n",
    "    'Lasso': lasso_pipe,\n",
    "    'Ridge': ridge_pipe}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the 5-fold cross validation for each model in a loop\n",
    "for i, model_name in enumerate(model_dict): \n",
    "    scores = cross_validate(model_dict[model_name], Xs_train, ys_train, cv = kf, return_train_score = True)\n",
    "    scores_df = tidy_scores(scores)\n",
    "    scores_df['model'] = model_name\n",
    "    scores_df = scores_df.set_index(\"model\", append=True)\n",
    "    scores_df = scores_df.swaplevel()\n",
    "    \n",
    "    if i == 0:\n",
    "        all_scores = scores_df\n",
    "    else:\n",
    "        all_scores = pd.concat([all_scores, scores_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score of optimal model\n",
    "# Mean values for the train and valid scores        \n",
    "display(all_scores.xs('mean', level=1, drop_level=False))\n",
    "# Std Dev. values for the train and valid scores\n",
    "display(all_scores.xs('sd', level=1, drop_level=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = linear_model.Lasso(alpha = best_lasso).fit(Xs_train, ys_train)\n",
    "coef = get_coefs(ms)[1:]\n",
    "feature = Xs_train.columns\n",
    "pd.DataFrame(coef, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = linear_model.Ridge(alpha = best_ridge).fit(Xs_train, ys_train)\n",
    "coef = get_coefs(m)[1:]\n",
    "feature = Xs_train.columns\n",
    "pd.DataFrame(coef, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "lasso_model = lasso_pipe.fit(Xs_train , ys_train)\n",
    "model_fit(lasso_model, Xs_train, ys_train, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit(lasso_model, Xs_test, ys_test, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = ridge_pipe.fit(Xs_train , ys_train)\n",
    "model_fit(ridge_model, Xs_train, ys_train, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit(ridge_model, Xs_test, ys_test, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edQVULLU7Fo3"
   },
   "source": [
    "## 4. Discussion and Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fi0yKPrQ7Fo4"
   },
   "source": [
    "*In this section you should provide a general overview of **your final model**, its **performance**, and **reliability**.* \n",
    "\n",
    "- You should discuss what the implications of your model are in terms of the included features, predictive performance, and anything else you think is relevant.\n",
    "\n",
    "- This should be written with a target audience of a NBC Universal executive who is with the show and university level mathematics but not necessarily someone who has taken a postgraduate statistical modeling course. \n",
    "\n",
    "- Your goal should be to convince this audience that your model is both accurate and useful.\n",
    "\n",
    "- Finally, you should include concrete recommendations on what NBC Universal should do to make their reunion episode a popular as possible.\n",
    "\n",
    "**Keep in mind that a negative result, i.e. a model that does not work well predictively, but that is well explained and justified in terms of why it failed will likely receive higher marks than a model with strong predictive performance but with poor or incorrect explanations / justifications.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add that we might want to condiser the timing of the episodes when they aired and that it could be cool to get data to see h\n",
    "# how much timing afffected the ratings\n",
    "\n",
    "# add idea about adjusted-IMDB rating based on the amount of votes (takes in the correlation between votes and rating)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References\n",
    "\n",
    "*In this section, you should present a list of external sources (except the course materials) that you used during the project, if any*\n",
    "\n",
    "- Additional data sources can be cited here, in addition to related python documentations, any other webpage sources that you benefited from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT reference to shrutepy here\n",
    "\n",
    "#refernce for trnasformation\n",
    "I.K. Yeo and R.A. Johnson, “A new family of power transformations to improve normality or symmetry.” Biometrika, 87(4), pp.954-959, (2000)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
