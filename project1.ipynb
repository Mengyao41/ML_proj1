{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOyOkJJA7Foy"
   },
   "source": [
    "# Machine Learning in Python - Group Project 1\n",
    "\n",
    "**Due Friday, March 10th by 16.00 pm.**\n",
    "\n",
    "*include contributors names here (such as Name1, Name2, ...)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqPpPRE37Fo0"
   },
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the instructions, we can see that the data is taken from this library (but then modified) so we install this\n",
    "# package and use the data for feature engineering!\n",
    "#!pip install schrutepy\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jb4qh-pi7Fo1"
   },
   "outputs": [],
   "source": [
    "# Add any additional libraries or submodules below\n",
    "\n",
    "# Data libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from schrutepy import schrutepy\n",
    "import itertools\n",
    "\n",
    "# Sentiment Analysis\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# download the vader lexicon \n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "\"\"\"Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for\n",
    "Sentiment Analysis of Social Media Text. Eighth International Conference on\n",
    "Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\"\"\"\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "\n",
    "# sklearn modules that are necessary\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjs5785u7Fo1"
   },
   "outputs": [],
   "source": [
    "# First we load the data \n",
    "data = pd.read_csv(\"the_office.csv\")\n",
    "# and our external data from SchrutePy. (Referenced at end)\n",
    "data_from_schrutepy = schrutepy.load_schrute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making sure that all the necessary libraries or submodules are uploaded here, please follow the given skeleton to create your project report. \n",
    "- Your completed assignment must follow this structure \n",
    "- You should not add or remove any of these sections, if you feel it is necessary you may add extra subsections within each (such as *2.1. Encoding*). \n",
    "\n",
    "**Do not forget to remove the instructions for each section in the final document.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K314dGEL7Fo1"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "*This section should include a brief introduction to the task and the data (assume this is a report you are delivering to a client).* \n",
    "\n",
    "- If you use any additional data sources, you should introduce them here and discuss why they were included.\n",
    "\n",
    "- Briefly outline the approaches being used and the conclusions that you are able to draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FINISH\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3P6Vdzbo7Fo2"
   },
   "source": [
    "## 2. Exploratory Data Analysis and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Arg9_dYE7Fo2"
   },
   "source": [
    "*Include a detailed discussion of the data with a particular emphasis on the features of the data that are relevant for the subsequent modeling.* \n",
    "\n",
    "- Including visualizations of the data is strongly encouraged - all code and plots must also be described in the write up. \n",
    "- Think carefully about whether each plot needs to be included in your final draft - your report should include figures but they should be as focused and impactful as possible.\n",
    "\n",
    "*Additionally, this section should also implement and describe any preprocessing / feature engineering of the data.*\n",
    "\n",
    "- Specifically, this should be any code that you use to generate new columns in the data frame `d`. All of this processing is explicitly meant to occur before we split the data in to training and testing subsets. \n",
    "- Processing that will be performed as part of an sklearn pipeline can be mentioned here but should be implemented in the following section.*\n",
    "\n",
    "**All code and figures should be accompanied by text that provides an overview / context to what is being done or presented.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will explore the given dataset and show what the SchrutePy dataset looks like, which we downloaded from the package SchrutePy (reference below). This was hinted at in the instructions and will give us extra data to make our model.\n",
    "\n",
    "Below is the given dataset from the_office.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the external dataset we use from SchrutePy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_schrutepy.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a quick overview of what our columns are, how the data is inputted and how the overall structure of the data is. This is important for us so we can create efficient code later!\n",
    "Now lets quickly describe our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.describe().T.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have 186 episodes in total, average rating of 8.25 (good show), several other parameters which will be important later when we feature engineer. We can see for example there is quite a large standard deviation in the amount of votes, which could skew our ratings. We also see our rating (target variable) has a standard deviation of 0.54, whereas the mean is 8.25. This shows there is enough spread of rating to make non-uniform predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The DataFrame: data, has ',data.isna().sum().sum(),' NaN values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about our external dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_schrutepy.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have the columns text and text_w_directions have NaN values. However, we will not be using these anyways since its just the script! So we do not have to worry about it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will no get our numerical data so we can visualize it:\n",
    "num_data = data.copy()\n",
    "num_data = num_data.drop(columns = ['episode_name', 'director', 'writer', 'air_date', 'main_chars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at the distribution of the columns. (except of course the season number, episode, and air date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(15,7), ncols=3, nrows=2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "#define our columns\n",
    "lst = ['imdb_rating','total_votes', 'n_lines', 'n_directions','n_words', 'n_speak_char']\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.histplot(data = num_data[lst[i]], bins=30, ax=ax)\n",
    "    ax.set_title(num_data[lst[i]].name)\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the distribution is important because it lets us see several hints such as the Skewness, Gaussian-ness, etc. We can see the IMDB rating follows what seems like a normal distribution, whereas total votes decreasing seemingly exponentially. While the mathematical relation isn't necessarily important for each of the variables, it is important to understand total votes is skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs above allow us to visualise the distributions of all of our numerical features. We can see that some features are skewed and others follow a normal gaussian distribution. From this information, we will implement a box cox transformation to transform all non-gaussian features to gaussian. This will be done in the feature engineering section. Doing so will improve the predictions made by our linear regression model further down the line. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before we move on to visualizing the correlation between variables, we want to see a breakdown of the relationship between writers and directors. After group discussion, we already know we want to include writers and directors in our model, but we need to make encoding of these variables. \n",
    "\n",
    "What is important to us is not the directors which had one hit wonders: the directors which had directed one episode and got lucky with a good rating. Maybe they are really good, but we are looking for consistency. The director for our final episode has to be consistently good.\n",
    "\n",
    "<b> We set a threshold to choose which directors to encode in our model:</b> we want directors to have directed at least ***5 shows*** and then from there select the ***best 5***  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get director dataframe which has their ratings and get a new column calculating the amount of times they directed\n",
    "director = data[['director', 'imdb_rating']].reindex(columns=list(['director', 'imdb_rating', 'freq']), fill_value=1)\n",
    "#calculates the sum\n",
    "director = director.groupby('director')[['director', 'imdb_rating', 'freq']].sum() \n",
    "director['mean_rating'] = round(director['imdb_rating'] / director['freq'], 2)\n",
    "# and sort the values for us on frequency (explained below) with a mean rating in the 75% percentile. \n",
    "director[director['freq']>=5].drop(columns = 'imdb_rating').sort_values(by='mean_rating', ascending=False).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that using this criteria, we get some of the consistnently best directors. Fortunately for us, we can see that all our directors with high frequency also have fairly good ratings, with Paul Feig having a very high rating consistently!\n",
    "\n",
    "Now lets do the same with writer. Here some times the writers collab. We will NOT split up the writers individually because we believe collaborations between writers can lead to better results than the writers individually together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get director dataframe which has their ratings and get a new column calculating the amount of times they directed\n",
    "writer = data[['writer', 'imdb_rating']].reindex(columns=list(['writer', 'imdb_rating', 'freq']), fill_value=1)\n",
    "#calculates the sum\n",
    "writer = writer.groupby('writer')[['writer', 'imdb_rating', 'freq']].sum() \n",
    "writer['mean_rating'] = round(writer['imdb_rating'] / writer['freq'], 2)\n",
    "# and sort the values for us on frequency (explained below) with a mean rating in the 75% percentile. \n",
    "writer[writer['freq']>=5].drop(columns = 'imdb_rating').sort_values(by='mean_rating', ascending=False).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same reasoning, we can find the best consistently best 5 writers which we will choose to encode!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done all of this: It is time to see the correlation between our variables so we can choose the best selection for our model to reduce feature dependence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap\n",
    "sns.set(rc={'figure.figsize': (14, 8)})\n",
    "sns.heatmap(num_data.corr(), annot = True, fmt = '.2f', linewidths = 2)\n",
    "plt.title(\"Correlation Heatmap for all \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some very important things to reduce colinearity/codependence:\n",
    "\n",
    "* n_lines and n_words is very highly correlated (obviously) so we will deal with that later. Other n_* lines are also slightly correlated so we will find a way to deal with those too.\n",
    "* imbd_rating and total_votes is also very highly correlated. \n",
    "    + We need to be careful here because this implies episodes with a large number of votes often is rated more highly. We cannot have number of votes as an input variable in our model because for a reunion episode, we obviosuly cannot say a priori the amount of votes we will get. To deal with this we will look deeper into the problem: We can adjust the imdb rating based on the amount of votes it got or choose to ignore the correlation if we find the relationship to be insignificant. More on this later...\n",
    "* The same is with rating and season number. However, since we cannot account for the season number in our reunion episode, we cannot do anything about this either. We could weight each season to imbd rating but do not believe this to be outside the scope of this project. It would be great for further research!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, lets look at how we can deal with number of ____ feautures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now want to see how the highly correlated values are relates:\n",
    "sns.pairplot(data[['n_lines','n_directions','n_words', 'n_speak_char']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a very clear __linear__ relationship between n_lines and n_words which is good for us since it means we can combine or eliminate one of those feautres. The other variables we do not see as strong relationship so we are hesitant to eliminate those. We will see if testing the model on a reduced parameter space will improve performance later, in which we can reduce some of these feautures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***FIX/Adjust BELOW***</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features we are eliminating (from the_office.csv) and why:\n",
    "* We will eliminate the column n_words since we found that n_lines and n_words were highly linearly correlated. Doing so will reduce collinearity in our final model. \n",
    "* We will remove season number, episode number and air_date because in our opinion, this should not have an affect on the reunion episode. We consider our personal experience and realize most of the office nowadays is watched through streaming services, which are independent of air date. We do however acknowledge that the rating could be dependent on air_time and season numbers, although marginally\n",
    "\n",
    "#### Features we will engineer:\n",
    "* We want to see how many times do the main characters talk during the episode and see if it effects the rating.\n",
    "* We want to OneHotEncode the writers and directors\n",
    "* OneHotEncode\n",
    "\n",
    "#### Will we adjust the IMDB rating according to the number of votes and season number to evenly weight the ratings for our model?\n",
    "As a group, we have decided to _______\n",
    "\n",
    "#### Finally, we will standardise all of our final numerical features\n",
    "This includes standardising the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 How many times do characters speak?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get this data from our external source: data_from_schrutepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data which is how many times does {Character} speak in each episode?\n",
    "speak_count = pd.DataFrame(data_from_schrutepy.groupby(by=['season','episode','character']).count()['text']).unstack()\n",
    "speak_count.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we now have 773 characters in the office, most of which do not speak: a lot of minor characters we do not wish to include in our model. So now lets filter the main characters only: this can be found in the the_office.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlate this to the main characters in this episode (from the_office.csv) and delete all minor characters\n",
    "a = data.main_chars.str.split(';')\n",
    "#this gets all the unique main characters from the_office.csv and save it as main_characters\n",
    "main_characters = list(itertools.chain.from_iterable(a))\n",
    "main_characters = pd.DataFrame(main_characters).drop_duplicates().reset_index()[0]\n",
    "\n",
    "# adjust the dataFrame index\n",
    "speak_count = speak_count.reset_index(names = ['episode','season', 'character'], col_level=1)\n",
    "# get just the main character\n",
    "speak_count_main_characters = speak_count['text'][main_characters].fillna(0)\n",
    "speak_count_main_characters.head()\n",
    "#then add the main characters to the model input data!\n",
    "data[speak_count_main_characters.columns] = speak_count_main_characters\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this worked out really well! We now have extra columns which shows how often the main characters speak in the show per episode!\n",
    "\n",
    "Below we can show a visual representation of how many times each main character speaks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data[main_characters])\n",
    "plt.xticks(rotation=45) # rotate x-labels 45 degrees\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are a lot of outliers. This shows some episodes focus on specific characters\n",
    "\n",
    "    ***REMOVE EPISODES WITH A FOCUS ON ONE CHARACTER*** AS further expansion on project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 One Hot Encode the writers and directors:\n",
    "\n",
    "It is now time to one hot encode the important writers and directors we found in the EDA section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define the directors and writers we are looking \n",
    "directors = ['Paul Feig','Ken Kwapis','Greg Daniels','Ken Whittingham','Charles McDougall']\n",
    "writers = ['Greg Daniels','Gene Stupnitsky;Lee Eisenberg','Michael Schur','Paul Lieberstein','B.J. Novak']\n",
    "\n",
    "# perform encoding on the director column\n",
    "encoded = pd.get_dummies(data['director'].apply(lambda x: x if x in directors else 'Other_director'))\n",
    "\n",
    "# merge the encoded dataframe with the original dataframe\n",
    "data = pd.concat([data, encoded], axis=1)\n",
    "\n",
    "# drop the original director column\n",
    "data.drop('director', axis=1, inplace=True)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets do the same but with the writers!\n",
    "# perform encoding on the director column\n",
    "encoded = pd.get_dummies(data['writer'].apply(lambda x: x if x in writers else 'Other_writer'))\n",
    "\n",
    "# merge the encoded dataframe with the original dataframe\n",
    "data = pd.concat([data, encoded], axis=1)\n",
    "\n",
    "# drop the original director column\n",
    "data.drop('writer', axis=1, inplace=True)\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Dropping our Unessecary Columns\n",
    "\n",
    "Its now time to get rid of season, episode,episode_name,total_votes\tand air_date so we can start to deal with the numbers of words and votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['season', 'episode', \n",
    "                          'episode_name','total_votes',\n",
    "                         'air_date', 'main_chars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 What to do with n_lines and n_words\n",
    "\n",
    "We know from our correlation matrix that these two are highly linearly correlated. That is why we will drop the n_words column (we could also have dropped n_lines instead). \n",
    "\n",
    "We will also know create a copy of our dataFrame to start working on the model with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['n_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5 Sentiment Score\n",
    "\n",
    "We now will run the sentiment analysis on the script per episode. This uses the Vader dictionary to determine how happy/neutral/sad a given script is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# define some text to analyze (fill the silences (nan) with  ' ')\n",
    "text = data_from_schrutepy.fillna(' ').groupby(['season', 'episode'])['text'].apply(' '.join)\n",
    "\n",
    "# analyze the sentiment of the text\n",
    "scores = [analyzer.polarity_scores(i) for i in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the neutral, positive and negative sentiment scores\n",
    "data['pos_score'] = [scores[i]['pos'] for i in range(len(scores))]\n",
    "data['neg_score'] = [scores[i]['neg'] for i in range(len(scores))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature Transformation\n",
    "\n",
    "The final step is to transform our data (using yeo-johnson) to improve our data normality and symmetry! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeo = PowerTransformer()\n",
    "data_scaled = yeo.fit_transform(data)\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df will be the dataframe we are working with now\n",
    "df = data.copy()\n",
    "df_scaled = data_scaled.copy()\n",
    "\n",
    "df.to_csv('final_office.csv')\n",
    "df_scaled.to_csv('final_office_scaled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad7J-Yw67Fo3"
   },
   "source": [
    "## 3. Model Fitting and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXdvOaZs7Fo3"
   },
   "source": [
    "*In this section you should detail your choice of model and describe the process used to refine and fit that model.*\n",
    "\n",
    "- You are strongly encouraged to explore many different modeling methods (e.g. linear regression, regression trees, lasso, etc.) but you should not include a detailed narrative of all of these attempts. \n",
    "- At most this section should mention the methods explored and why they were rejected - most of your effort should go into describing the model you are using and your process for tuning and validatin it.\n",
    "\n",
    "*For example if you considered a linear regression model, a classification tree, and a lasso model and ultimately settled on the linear regression approach then you should mention that other two approaches were tried but do not include any of the code or any in depth discussion of these models beyond why they were rejected. This section should then detail is the development of the linear regression model in terms of features used, interactions considered, and any additional tuning and validation which ultimately led to your final model.* \n",
    "\n",
    "**This section should also include the full implementation of your final model, including all necessary validation. As with figures, any included code must also be addressed in the text of the document.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In GIT there is the final csv file called \n",
    "filneame = './final_office.csv'\n",
    "# in case you guys do not want to run the entire sections above\n",
    "\n",
    "# to inverse the scaling we used: after prediction do: \n",
    "# yeo.inverse_transform(prediction_data)\n",
    "# and be careful! of the column names!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edQVULLU7Fo3"
   },
   "source": [
    "## 4. Discussion and Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fi0yKPrQ7Fo4"
   },
   "source": [
    "*In this section you should provide a general overview of **your final model**, its **performance**, and **reliability**.* \n",
    "\n",
    "- You should discuss what the implications of your model are in terms of the included features, predictive performance, and anything else you think is relevant.\n",
    "\n",
    "- This should be written with a target audience of a NBC Universal executive who is with the show and university level mathematics but not necessarily someone who has taken a postgraduate statistical modeling course. \n",
    "\n",
    "- Your goal should be to convince this audience that your model is both accurate and useful.\n",
    "\n",
    "- Finally, you should include concrete recommendations on what NBC Universal should do to make their reunion episode a popular as possible.\n",
    "\n",
    "**Keep in mind that a negative result, i.e. a model that does not work well predictively, but that is well explained and justified in terms of why it failed will likely receive higher marks than a model with strong predictive performance but with poor or incorrect explanations / justifications.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add that we might want to condiser the timing of the episodes when they aired and that it could be cool to get data to see h\n",
    "# how much timing afffected the ratings\n",
    "\n",
    "# add idea about adjusted-IMDB rating based on the amount of votes (takes in the correlation between votes and rating)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References\n",
    "\n",
    "*In this section, you should present a list of external sources (except the course materials) that you used during the project, if any*\n",
    "\n",
    "- Additional data sources can be cited here, in addition to related python documentations, any other webpage sources that you benefited from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT reference to shrutepy here\n",
    "\n",
    "#refernce for trnasformation\n",
    "I.K. Yeo and R.A. Johnson, “A new family of power transformations to improve normality or symmetry.” Biometrika, 87(4), pp.954-959, (2000)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
